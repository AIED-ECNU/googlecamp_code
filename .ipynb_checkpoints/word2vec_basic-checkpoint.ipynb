{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用TensorFlow训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入Python包，__future__ 包是为了扩展Python当前版本对代码的兼容性\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入我们需要使用的包\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import jieba\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量的训练步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量的训练步骤主要分为六个步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步：文本预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #定义好需要处理的文件路径\n",
    "    train_data_path = 'train_data/word/all_review.txt'\n",
    "    stopwords_path = 'train_data/word/stopwords.txt'\n",
    "\n",
    "    # 定义一个空数组，用来存放停用词\n",
    "    stopwords = []\n",
    "\n",
    "    #读取stopwords.txt中的所有内容\n",
    "    fopen = open(stopwords_path, \"r\")\n",
    "    # 读取文件中的每一行数据\n",
    "    for line in fopen.readlines():\n",
    "        stopwords.append(line.strip())\n",
    "    #关闭文档\n",
    "    fopen.close()\n",
    "    print('共{n}个停用词'.format(n=len(stopwords)))\n",
    "\n",
    "    #读取训练的语料\n",
    "\n",
    "    train_data = []  # 存放读取出来的训练语料\n",
    "\n",
    "    data_open = open(train_data_path, \"r\")\n",
    "    line = data_open.readline()\n",
    "    while line:\n",
    "        while '\\n' in line:\n",
    "            line = line.replace('\\n', '')\n",
    "        while ' ' in line:\n",
    "            line = line.replace(' ', '')\n",
    "        if len(line) > 0:  # 如果句子长度大于0，也就是句子不为空\n",
    "            datawords = list(jieba.cut(line, cut_all=False))\n",
    "            # 去除停用词,new_datawords存放每一行去除停用词之后的文本\n",
    "            new_datawords=[]\n",
    "            for word in datawords:\n",
    "                if word not in stopwords:\n",
    "                    new_datawords.append(word)\n",
    "                else:\n",
    "                    pass\n",
    "            train_data.extend(new_datawords)\n",
    "        line = data_open.readline()\n",
    "    # 关闭文档\n",
    "    data_open.close()\n",
    "    print('一共分词{n}个'.format(n=len(train_data)))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 获取预处理好的文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共508个停用词\n",
      "一共分词1373204个\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：建立词典，将比较少出现的词汇用UNK代替"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先我们需要设置这个词典的词汇量的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 40000 # 设置词汇量的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_trainset(traindata):\n",
    "    count = [['UNK', -1]]\n",
    "\n",
    "    count.extend(collections.Counter(traindata).most_common(vocabulary_size - 1))\n",
    "    # count得到的格式为：[['UNK', -1], ('学习', 22596), ('我', 21683), ('老师', 17704), ('课程', 17268)]\n",
    "    print(\"count\", len(count))\n",
    "\n",
    "    dictionary = dict() # 定义一个字典,格式为：{'UNK': 0, '看书': 1, '学习': 2, '我': 3, '老师': 4, '课程': 5}\n",
    "\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data_index = list()\n",
    "    unk_count = 0\n",
    "    for word in traindata:\n",
    "        if word in dictionary:\n",
    "            #找出traindata中的词在dictionary这个排好序号的字典中的索引，\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count += 1\n",
    "        # 把索引号存放在data_index中\n",
    "        data_index.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    # dictionary字典中的值和keys调换顺序\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    # print('reverse_dictionary',reverse_dictionary)\n",
    "    return data_index, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 40000\n"
     ]
    }
   ],
   "source": [
    "data, count, dictionary, reverse_dictionary=construct_trainset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将词典存储为pkl文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output1 = open('model/dictionary.pkl', 'wb')\n",
    "\n",
    "output2 = open('model/reverse_dictionary.pkl', 'wb')\n",
    "\n",
    "pickle.dump(dictionary, output1)\n",
    "pickle.dump(reverse_dictionary, output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#删除train_data节省内存\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看训练数据中频率出现最多的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 8062], ('学习', 22596), ('老师', 17704), ('课程', 17268), ('很', 16089)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：构建训练函数，生成Word2Vec的训练样本，使用skip-gram模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch_size: 每个训练批次的数据量\n",
    "- num_skips: 每个单词生成的样本数量，不能超过skip_window的两倍，并且必须是batch_size的整数倍\n",
    "- skip_window: 单词最远可以联系的距离，设置为1则表示当前单词只考虑前后两个单词之间的关系，也称为滑窗的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index #声明为全局变量，方便后期多次使用\n",
    "     #使用Python中的断言函数，提前对输入的参数进行判别，防止后期出bug而难以寻找原因\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    #创建一个batch_size大小的数组，数据类型为int32类型，数值随机\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    #数据维度为[batch_size,1]\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    #入队的长度\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    #创建双向队列。最大长度为span\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    #对双向队列填入初始值\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    #进入第一层循环，i表示第几次入双向队列\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # 定义buffer中第skip_window个单词是目标\n",
    "        \n",
    "        #定义生成样本时需要避免的单词，因为我们要预测的是语境单词，不包括目标单词本身，因此列表开始包括第skip_window个单词\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            #因为该语境单词已经被使用过了，因此将其添加到需要避免的单词库中\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window] #目标词汇\n",
    "            labels[i * num_skips + j, 0] = buffer[target] #语境词汇\n",
    "        #此时buffer已经填满，后续的数据会覆盖掉前面的数据\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels # return:返回每个批次的样本以及对应的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取每个批次的样本以及对应的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目标单词：开       对应编号为：       804   对应的语境单词为:        ﻿    编号为 24781\n",
      "目标单词：开       对应编号为：       804   对应的语境单词为:        一门    编号为 118\n",
      "目标单词：一门       对应编号为：       118   对应的语境单词为:        开    编号为 804\n",
      "目标单词：一门       对应编号为：       118   对应的语境单词为:        进阶    编号为 3537\n",
      "目标单词：进阶       对应编号为：       3537   对应的语境单词为:        一门    编号为 118\n",
      "目标单词：进阶       对应编号为：       3537   对应的语境单词为:        版    编号为 704\n",
      "目标单词：版       对应编号为：       704   对应的语境单词为:        进阶    编号为 3537\n",
      "目标单词：版       对应编号为：       704   对应的语境单词为:        课程    编号为 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(\"目标单词：\"+reverse_dictionary[batch[i]]+\"对应编号为：\".center(20)+str(batch[i])+\"   对应的语境单词为: \".ljust(20)+reverse_dictionary[labels[i,0]]+\"    编号为\",labels[i,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：构建训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #训练样本的批次大小\n",
    "embedding_size = 128 #单词转化为词向量的维度\n",
    "skip_window = 5 #单词可以联系到的最远距离\n",
    "num_skips = 2 #每个目标单词提取的样本数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型验证的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size = 2      #验证的单词数，切记这个数字要和len(valid_word)对应，要不然会报错哦\n",
    "valid_window = 100  #  #指验证单词只从频数最高的前100个单词中进行抽取\n",
    "num_sampled = 64    # 训练时用来做负样本的噪声单词的数量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_word = ['课程','教学']\n",
    "valid_examples =[dictionary[li] for li in valid_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始定义Skip-Gram Word2Vec模型的网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建一个graph作为默认的计算图，同时为输入数据和标签申请占位符，并将验证样例的随机数保存成TensorFlow的常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    #选择运行的device为CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        \n",
    "        # tf.Variable生成随机数，tf.random_uniform平均分布，#单词大小为40000，向量维度为128，随机采样在（-1，1）之间的浮点数\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "         #使用tf.nn.embedding_lookup()函数查找train_inputs对应的向量embed\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "         #优化目标选择NCE loss\n",
    "        #使用截断正太函数初始化NCE损失的权重,偏重初始化为0\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32)     \n",
    "        \n",
    "        #计算学习出的embedding在训练数据集上的loss，并使用tf.reduce_mean()函数进行汇总\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         inputs=embed,\n",
    "                                         labels=train_labels,\n",
    "                                         num_sampled=num_sampled,\n",
    "                                         num_classes=vocabulary_size))\n",
    "        \n",
    "        #  #定义优化器为SGD，且学习率设置为1.0.然后计算嵌入向量embeddings的L2范数norm，并计算出标准化后的normalized_embeddings\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "         #计算验证单词的嵌入向量与词汇表中所有单词的相似性\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "        #定义参数的初始化\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # 保存参数\n",
    "        saver = tf.train.Saver()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100000 #迭代训练次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个会话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化完成\n",
      "第0轮迭代后的损失为：247.09677124023438\n",
      "与单词 课程 最相似的：  课程, 套进去, 太短, 局外人, 赞赞, 脱离实际, 苏菲, 不减,\n",
      "与单词 教学 最相似的：  教学, 资治通鉴, 艺术修养, 娃子, 重新整理, 口诀, theybecomeaburdentous, 3dmax,\n",
      "第2000轮迭代后的损失为：105.8302780828476\n",
      "第4000轮迭代后的损失为：44.89962465238571\n",
      "第6000轮迭代后的损失为：29.547050262451172\n",
      "第8000轮迭代后的损失为：19.13878569817543\n",
      "第10000轮迭代后的损失为：13.26428614282608\n",
      "与单词 课程 最相似的：  课程, 急急, 发发, 课, 发送到, ２, 学习, ★,\n",
      "与单词 教学 最相似的：  教学, 急急, 发发, ★, 发送到, 亿, 课程, 资治通鉴,\n",
      "第12000轮迭代后的损失为：11.292211020946503\n",
      "第14000轮迭代后的损失为：8.032667596101762\n",
      "第16000轮迭代后的损失为：7.427209845066071\n",
      "第18000轮迭代后的损失为：6.662233720302582\n",
      "第20000轮迭代后的损失为：8.250693349957466\n",
      "与单词 课程 最相似的：  课程, 课, 小手, 急急, 学习, 很, 教学, 发发,\n",
      "与单词 教学 最相似的：  教学, 急急, 课程, 学习, 小手, 发发, ★, 亿,\n",
      "第22000轮迭代后的损失为：7.368925020650029\n",
      "第24000轮迭代后的损失为：6.93391961979866\n",
      "第26000轮迭代后的损失为：6.267407975196838\n",
      "第28000轮迭代后的损失为：5.832287271380425\n",
      "第30000轮迭代后的损失为：5.283919135928154\n",
      "与单词 课程 最相似的：  课程, 课, 学习, 急急, 小手, 教学, 这门, 很,\n",
      "与单词 教学 最相似的：  教学, 课程, 急急, 学习, 小手, 发发, ★, 教育,\n",
      "第32000轮迭代后的损失为：5.136579441308975\n",
      "第34000轮迭代后的损失为：5.131040546178817\n",
      "第36000轮迭代后的损失为：5.090664197087288\n",
      "第38000轮迭代后的损失为：5.698519450128078\n",
      "第40000轮迭代后的损失为：5.664203164532781\n",
      "与单词 课程 最相似的：  课程, 课, 小手, 学习, 急急, 教学, 很, 好好,\n",
      "与单词 教学 最相似的：  教学, 好好, 课程, 小手, 急急, 学习, 发发, 亿,\n",
      "第42000轮迭代后的损失为：5.428038918673992\n",
      "第44000轮迭代后的损失为：5.325086282491684\n",
      "第46000轮迭代后的损失为：5.202044887304306\n",
      "第48000轮迭代后的损失为：5.005449159622192\n",
      "第50000轮迭代后的损失为：4.974032434105873\n",
      "与单词 课程 最相似的：  课程, 课, 学习, 小手, 急急, 教学, 好好, 这门,\n",
      "与单词 教学 最相似的：  教学, 好好, 课程, 急急, 教育, 小手, 学习, 技术,\n",
      "第52000轮迭代后的损失为：4.965675091862678\n",
      "第54000轮迭代后的损失为：4.961235743522644\n",
      "第56000轮迭代后的损失为：5.140567743062973\n",
      "第58000轮迭代后的损失为：5.218513732001186\n",
      "第60000轮迭代后的损失为：5.042600801706314\n",
      "与单词 课程 最相似的：  课程, 课, 小手, 学习, 教学, 觉得, 很, 急急,\n",
      "与单词 教学 最相似的：  教学, 学习, 小手, 课程, 急急, 教育, 技术, 信息化,\n",
      "第62000轮迭代后的损失为：5.081171849131584\n",
      "第64000轮迭代后的损失为：5.012511544108391\n",
      "第66000轮迭代后的损失为：4.92976152151823\n",
      "第68000轮迭代后的损失为：4.9264820677042005\n",
      "第70000轮迭代后的损失为：4.919787620544434\n",
      "与单词 课程 最相似的：  课程, 课, 教学, 学习, 小手, 急急, 觉得, 这门,\n",
      "与单词 教学 最相似的：  教学, 课程, 急急, 教育, 小手, 学习, 信息化, 中,\n",
      "第72000轮迭代后的损失为：4.919915591716767\n",
      "第74000轮迭代后的损失为：4.99378508079052\n",
      "第76000轮迭代后的损失为：4.941950102567673\n",
      "第78000轮迭代后的损失为：4.844858775004744\n",
      "第80000轮迭代后的损失为：5.001870033144951\n",
      "与单词 课程 最相似的：  课程, 课, 小手, 急急, 学习, 觉得, 教学, 内容,\n",
      "与单词 教学 最相似的：  教学, 教育, 运用, 学习, 技术, 课程, 急急, 信息化,\n",
      "第82000轮迭代后的损失为：4.954636410593986\n",
      "第84000轮迭代后的损失为：4.869717746019363\n",
      "第86000轮迭代后的损失为：4.922103014111519\n",
      "第88000轮迭代后的损失为：4.887429806828499\n",
      "第90000轮迭代后的损失为：4.887825344085694\n",
      "与单词 课程 最相似的：  课程, 课, 急急, 小手, 教学, 学习, 觉得, 设计,\n",
      "与单词 教学 最相似的：  教学, 学习, 教育, 课程, 急急, 运用, 信息化, 设计,\n",
      "第92000轮迭代后的损失为：4.936569036006928\n",
      "第94000轮迭代后的损失为：4.935733313061297\n",
      "第96000轮迭代后的损失为：4.82562877471745\n",
      "第98000轮迭代后的损失为：4.930936639547348\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"初始化完成\")\n",
    "    average_loss = 0 #计算误差\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window) #调用生成训练数据函数生成一组batch和label\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels} #待填充的数据\n",
    "\n",
    "        #启动回话，运行优化器optimizer和损失计算函数，并填充数据\n",
    "        optimizer_trained, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        #统计NCE损失\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        #为了方便查看训练过程，每2000次计算一下损失并显示出来\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"第{}轮迭代后的损失为：{}\".format(step,average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "        #每10000次迭代，计算一次验证单词与全部词的相似度，并将于验证单词最相似的前8个词\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval() #计算向量\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]  #得到对应的验证词\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[:top_k] #计算每一个验证单词相似度最接近的前8个词\n",
    "                log_str = \"与单词 {} 最相似的： \".format(str(valid_word))\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]  #相似度高的词\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    # 保存训练好的模型\n",
    "    saver.save(session, \"model/my-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第六步：词向量可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='images/tsne.png', fonts=None):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"标签数超过了嵌入向量的个数！\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     fontproperties=fonts,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "\n",
    "    plt.savefig(filename, dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "\n",
    "    # 为了在图片上能显示出中文\n",
    "    font = FontProperties(fname=r\"train_data/simsun/simsun.ttc\", size=14)\n",
    "\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    plot_only = 500 # 取500个词打印\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "    plot_with_labels(low_dim_embs, labels, fonts=font)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Please install sklearn, matplotlib, and scipy to visualize embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
