{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用长短时记忆模型（LSTM）做情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入Python包，__future__ 包是为了扩展Python当前版本对代码的兼容性\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 引入所需要的包\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import jieba\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预处理停用词\n",
    "def StopWords():\n",
    "    with open('train_data/stopwords.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    stopWords = []\n",
    "    for line in lines:\n",
    "        stopWords.append(line.strip())\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取数据，并去除停用词\n",
    "def read_data_file(filepath):\n",
    "    wordList = []\n",
    "    sentence = []\n",
    "    with open(filepath,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        words = list(jieba.cut(line.strip(), cut_all=False))\n",
    "        for word in words:\n",
    "            # 如果词汇不属于停用词，则把词放到wordList中\n",
    "            if word not in stopWords:\n",
    "                wordList.append(word)\n",
    "        sentence.append(wordList)\n",
    "        wordList = []\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将数据转换成词向量，返回词向量矩阵\n",
    "def words2Array(lineList):\n",
    "    linesArray=[]\n",
    "    wordsArray=[]\n",
    "    steps = []\n",
    "    for line in lineList:\n",
    "        t = 0\n",
    "        p = 0\n",
    "        for i in range(MAX_SIZE):\n",
    "            # 如果小于MAX_SIZE 则用0.0的矩阵补齐\n",
    "            if i<len(line):\n",
    "                try:\n",
    "                    wordsArray.append(word2vec_model.wv.word_vec(line[i]))\n",
    "                    p = p + 1\n",
    "                except KeyError:\n",
    "                    # 如果在词向量模型中没有找到对应词的向量\n",
    "                    t=t+1\n",
    "                    continue\n",
    "            else:\n",
    "               wordsArray.append(np.array([0.0]*vector_size))\n",
    "\n",
    "        for i in range(t):\n",
    "            wordsArray.append(np.array([0.0]*vector_size))\n",
    "        steps.append(p)\n",
    "        linesArray.append(wordsArray)\n",
    "        wordsArray = []\n",
    "    linesArray = np.array(linesArray)\n",
    "    steps = np.array(steps)\n",
    "    return linesArray, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 打乱数据集\n",
    "def convert2Data(posArray, negArray, posStep, negStep):\n",
    "    randIt = []\n",
    "    data = []\n",
    "    steps = []\n",
    "    labels = []\n",
    "    for i in range(len(posArray)):\n",
    "        # 制作数据集 积极的标签为one-hot表示方法 [1,0]\n",
    "        randIt.append([posArray[i], posStep[i], [1,0]])\n",
    "    for i in range(len(negArray)):\n",
    "        # 制作数据集 消极的标签为one-hot表示方法 [0,1]\n",
    "        randIt.append([negArray[i], negStep[i], [0,1]])\n",
    "    shuffle(randIt)\n",
    "    for i in range(len(randIt)):\n",
    "        data.append(randIt[i][0])\n",
    "        steps.append(randIt[i][1])\n",
    "        labels.append(randIt[i][2])\n",
    "    data = np.array(data)\n",
    "    steps = np.array(steps)\n",
    "    return data, steps, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步，构建训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建训练数据集\n",
    "def construct_train_data(posPath,negPath):\n",
    "    #获取词汇，返回类型为[[word1,word2...],[word1,word2...],...]\n",
    "    pos = read_data_file(posPath)\n",
    "    print(\"The positive data's length is :\",len(pos))\n",
    "    neg = read_data_file(negPath)\n",
    "    print(\"The negative data's length is :\",len(neg))\n",
    "    #将评价数据转换为矩阵，返回类型为array\n",
    "    posArray, posSteps = words2Array(pos)\n",
    "    negArray, negSteps = words2Array(neg)\n",
    "    #将积极数据和消极数据混合在一起打乱，制作数据集\n",
    "    Data, Steps, Labels = convert2Data(posArray, negArray, posSteps, negSteps)\n",
    "    return Data, Steps, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取停用词\n",
    "stopWords = StopWords()\n",
    "word2vec_path = 'word2vec/word2vec.pkl' # 词向量地址\n",
    "file = open(word2vec_path, 'rb')\n",
    "word2vec_model = pickle.load(file )\n",
    "vector_size=word2vec_model.vector_size # 词向量的维度\n",
    "MAX_SIZE=25 # 句子的统一长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/z4/zf4yk5gs7r98p__16wj4jzgw0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.913 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive data's length is : 638\n",
      "The negative data's length is : 294\n",
      "In test data:\n",
      "The positive data's length is : 102\n",
      "The negative data's length is : 17\n"
     ]
    }
   ],
   "source": [
    "# 打印数据,看一下数据的条数\n",
    "print(\"In train data:\")\n",
    "trainData, trainSteps, trainLabels = construct_train_data('train_data/pos.txt',\n",
    "                                              'train_data/neg.txt')\n",
    "# print(trainData)\n",
    "print(\"In test data:\")\n",
    "testData, testSteps, testLabels = construct_train_data('train_data/test_pos.txt',\n",
    "                                           'train_data/test_neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将训练标签变成numpy数组\n",
    "trainLabels = np.array(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del word2vec_model #删除词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看各个数据的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------打印数据结果----------\n",
      "训练数据的形状: (932, 25, 100)\n",
      "测试数据的形状: (119, 25, 100)\n",
      "训练数据步数的形状: (932,)\n",
      "测试数据步数的形状: (119,)\n",
      "训练标签的形状: (932, 2)\n",
      "测试标签的形状: (119, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*10+'打印数据结果'+\"-\"*10)\n",
    "print(\"训练数据的形状:\",trainData.shape)\n",
    "print(\"测试数据的形状:\",testData.shape)\n",
    "print(\"训练数据步数的形状:\",trainSteps.shape)\n",
    "print(\"测试数据步数的形状:\",testSteps.shape)\n",
    "print(\"训练标签的形状:\",trainLabels.shape)\n",
    "print(\"测试标签的形状:\",np.array(testLabels).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步 构造神经网络计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "num_nodes = 128  # 隐藏神经元的数量\n",
    "batch_size = 16 # 一次喂给训练器的数据条数\n",
    "output_size = 2 # 输出的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-b1513d9a1493>:39: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 构造计算图\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(None, MAX_SIZE, vector_size),name='x')\n",
    "    tf_train_steps = tf.placeholder(tf.int32, shape=(None),name='x_step')\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, output_size))\n",
    "\n",
    "    # tf_test_dataset = tf.constant(testData, tf.float32,name='input_x')\n",
    "    # tf_test_steps = tf.constant(testSteps, tf.int32,name='steps')\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape=(None, MAX_SIZE, vector_size), name='input_x')\n",
    "    tf_test_steps = tf.placeholder(tf.int32, shape=(None), name='steps')\n",
    "\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=num_nodes,\n",
    "                                             state_is_tuple=True)\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes // 2], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.truncated_normal([num_nodes // 2], stddev=0.1))\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([num_nodes // 2, 2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.truncated_normal([2], stddev=0.1))\n",
    "\n",
    "\n",
    "    def model(dataset, steps):\n",
    "        outputs, last_states = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                                 dtype=tf.float32,\n",
    "                                                 sequence_length=steps,\n",
    "                                                 inputs=dataset)\n",
    "        hidden = last_states[-1]\n",
    "\n",
    "        hidden = tf.matmul(hidden, w1) + b1\n",
    "        logits = tf.matmul(hidden, w2) + b2\n",
    "        return logits\n",
    "\n",
    "\n",
    "    train_logits = model(tf_train_dataset, tf_train_steps)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,\n",
    "                                                logits=train_logits))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, tf_test_steps))\n",
    "\n",
    "    tf.add_to_collection('pred_network', test_prediction) #用于加载模型获取要预测的网络结构\n",
    "\n",
    "    #保存模型\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "The step is: 200\n",
      "In train data,the loss is:0.5365\n",
      "In test data,the accuracy is:92.44%\n",
      "The step is: 400\n",
      "In train data,the loss is:0.4240\n",
      "In test data,the accuracy is:93.28%\n",
      "The step is: 600\n",
      "In train data,the loss is:0.3727\n",
      "In test data,the accuracy is:93.28%\n",
      "The step is: 800\n",
      "In train data,the loss is:0.3479\n",
      "In test data,the accuracy is:93.28%\n",
      "保存模型成功\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000 # 迭代次数\n",
    "summary_frequency = 200\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    print('Initialized')\n",
    "    session.run(init_op)\n",
    "\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (len(trainLabels)-batch_size)\n",
    "        feed_dict={tf_train_dataset:trainData[offset:offset + batch_size],\n",
    "                   tf_train_labels:trainLabels[offset:offset + batch_size],\n",
    "                   tf_train_steps:trainSteps[offset:offset + batch_size]}\n",
    "        _, l = session.run([optimizer,loss],\n",
    "                           feed_dict = feed_dict)\n",
    "        mean_loss += l\n",
    "        if step >0 and step % summary_frequency == 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "            print(\"The step is: %d\"%(step))\n",
    "            print(\"In train data,the loss is:%.4f\"%(mean_loss))\n",
    "            mean_loss = 0\n",
    "            acrc = 0\n",
    "            prediction = session.run(test_prediction,feed_dict={tf_test_dataset:testData,tf_test_steps:testSteps})\n",
    "            for i in range(len(prediction)):\n",
    "                if prediction[i][testLabels[i].index(1)] > 0.5:\n",
    "                    acrc = acrc + 1\n",
    "            print(\"In test data,the accuracy is:%.2f%%\"%((acrc/len(testLabels))*100))\n",
    "    saver.save(session, \"model/model-senti\")\n",
    "    print('保存模型成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
